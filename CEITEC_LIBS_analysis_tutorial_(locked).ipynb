{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryb4666/TugasAkhirFMIPAUSK/blob/main/CEITEC_LIBS_analysis_tutorial_(locked).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCwgEzmgpn6J"
      },
      "source": [
        "# **Classification of Laser-induced breakdown spectroscopy (LIBS) datasets**\n",
        "\n",
        "authors:\n",
        "- Jakub Hruška <jhruska@mail.muni.cz>\n",
        "- Jakub Vrábel <jakub.vrabel@ceitec.vutbr.cz>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rR9WhoQ5_ON"
      },
      "source": [
        "# 1) Introduction and dataset loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbIzFtTyipT9"
      },
      "source": [
        "## Environment setup\n",
        "\n",
        "This notebook leads you through basic data analysis of Laser-Induced Breakdown Spectroscopy (LIBS) data. It sheds light on data processing and machine learning with primary focus on classification.\n",
        "\n",
        "---\n",
        "\n",
        "At first you must copy this _read-only_ file into your Google Drive. Then you will be able to run and modify the cells.\n",
        "\n",
        "_File -> Save a copy in Drive..._  \n",
        "_File -> Locate in Drive_ (to find the location of your newly created copy)\n",
        "\n",
        "---\n",
        "\n",
        "In the next steps you prepare an environment on your Google Drive for convenient work with this tutorial.\n",
        "\n",
        "Start with creating a Colab runtime environment and mount your Google Drive into this environment. Just run the following cell and go through the auth procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb5Grzym0wKb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2UdPezbaOwm"
      },
      "source": [
        "In _Files_ tab (left menu) you can now see your Google Drive mounted at `drive/My Drive/`. For easier manipulation in the future change your working directory (currently `/content/`) using module [os](https://docs.python.org/3.6/library/os.html). Change `wd_path` variable to a path to a directory in your Google Drive where you want to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mqGKsLGaRvY"
      },
      "source": [
        "import os\n",
        "\n",
        "wd_path = \"COLAB_LIBS/contest\"\n",
        "os.chdir(\"/content/drive/My Drive/\" + wd_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL7iKldlaW-C"
      },
      "source": [
        "_Note:_ There are more options how to upload files into Google Colab. One possible alternative is direct upload from your local drive. `files.upload()` command allows you to upload files directly. It saves them into your current working directory (it can be within your mounted Google Drive also) and returns a dictionary of the files which were uploaded. It is sometimes needed (in some browsers) to run the cell twice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4Jj9_eBanHB"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMcPQDG_3Iuu"
      },
      "source": [
        "Both ways (mounting your GDrive or direct upload), you have to repeat the process every time you restart your runtime enviroment. An exhaustive description of how to load and save files in Google Colab is [here](https://colab.research.google.com/notebooks/io.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbVzwNN06EwA"
      },
      "source": [
        "Now you need to get and install our tool for loading EMSLIBS-contest datasets from [GitHub](https://github.com/JVrabel/EMSLIBS_contest)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr8LVlz97NJN"
      },
      "source": [
        "!git clone https://github.com/JVrabel/EMSLIBS_contest.git\n",
        "os.chdir(\"EMSLIBS_contest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MovPv1TKWvpP"
      },
      "source": [
        "If you already have done this once, you don't have to clone the GitHub repository again. Just check you are in correct working directory. If there is an upgrade in the tool and you want to update your copy, just use `!git pull` command in the `EMSLIBS_contest` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl8ayaNmbvky"
      },
      "source": [
        "os.chdir(\"EMSLIBS_contest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_I0_nCWybN3"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yuk_y4g-BTX"
      },
      "source": [
        "Then you simply import the tool. Please, take a look into the in-code [documentation](https://github.com/JVrabel/EMSLIBS_contest/blob/master/load_scripts/h5_load_contest.py) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw-2YBH78Ab_"
      },
      "source": [
        "import load_scripts.h5_load_contest as emslibs_load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMsUxRTc-rDO"
      },
      "source": [
        "Finally you need to upload the dataset file into the `datasets/` subfolder. In this tutorial you need only `train.h5` dataset from this [link](https://ndownloader.figstatic.com/files/20065616). You can find more info about the dataset (including original _test_ dataset) in this [paper](https://www.nature.com/articles/s41597-020-0396-8). After storing it in the `datasets/` subfolder at your GDrive, rename it to `contest_TRAIN.h5` please.\n",
        "\n",
        "You can do this all using this single-line command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T6R2N0R9WIT"
      },
      "source": [
        "!wget -O datasets/contest_TRAIN.h5 https://ndownloader.figstatic.com/files/20065616"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3kd_vAjbAfD"
      },
      "source": [
        "## Read the files\n",
        "\n",
        "We use [pandas](https://pandas.pydata.org/) and [numpy](https://numpy.org/) packages for manipulating with our datasets in this tutorial and in the EMSLIBS-contest loading tool.\n",
        "\n",
        "Check the documentation of Pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) structures, used to store and manipulate the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtOpYyefhfvc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMl-EViQOI0T"
      },
      "source": [
        "Reading the .h5 file can take a while."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSEGSK_4L4bE"
      },
      "source": [
        "path_to_ds = 'datasets/'\n",
        "spectra_count = 100\n",
        "wavelengths = emslibs_load.load_wavelengths(path_to_ds)\n",
        "wavelengths = np.round(wavelengths, 2)\n",
        "x_raw = emslibs_load.load_train_data(path_to_ds, spectra_count)\n",
        "x_raw = pd.DataFrame(x_raw, columns=wavelengths)\n",
        "y_raw = emslibs_load.load_train_labels(path_to_ds, spectra_count)\n",
        "y_raw = pd.Series(y_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j058GmKU_t-"
      },
      "source": [
        "For sake of different models comparison it's necessary to split the train dataset into two: _train_ and _validation_. You should use only _train_ dataset for training a model and _validation_ (_val_) dataset for evaluation of the model.\n",
        "\n",
        "This task (contest from EMSLIBS 2019) has quite specific test dataset (see the paper mentioned before for more info), so ideally your _train-validation_ split should reflect this too. In this simple tutorial, we ignore it and use basic `train_test_split()` from _Scikit-learn_. However, keep in mind that the results are much better in this case, because the datasets are much more similar compared to the original _train-test_ split in the contest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O4i-B_dVAhB"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUucobXwGfS"
      },
      "source": [
        "x_train_raw, x_val_raw, y_train_raw, y_val_raw = train_test_split(\n",
        "    x_raw, y_raw, train_size=0.7, random_state=42)\n",
        "del x_raw, y_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTIhyBMnNTkG"
      },
      "source": [
        "# 2) Exploring the dataset\n",
        "\n",
        "Now we have 4 DataFrames - two datasets (training and validation) divided into inputs `x_` (spectra) and desired outputs `y_` (labels of classes). Plus we have stored values of wavelengths (in nm) presented in the datasets.\n",
        "\n",
        "At first we check [shape](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html) of our datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ_hQ-UOPXOm"
      },
      "source": [
        "print(\"Train\\t\\tx:{}, y:{}\".format(x_train_raw.shape, y_train_raw.shape))\n",
        "print(\"Validation\\tx:{}, y:{}\".format(x_val_raw.shape, y_val_raw.shape))\n",
        "print(\"Wavelengths:\\t{}\".format(wavelengths.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD0C8fIfQ6rP"
      },
      "source": [
        "You can see that the train dataset contains 7000 samples with 40 002 features (variables, dimensions, attributes, ...) each. Every feature corresponds to certain wavelength. Validation dataset consists of 3000 samples of same dimension as training dataset.\n",
        "\n",
        "Let's look directly into datasets using [`.head()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) and [`.tail()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html) functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vrecp8tQ5yA"
      },
      "source": [
        "print(\"Train:\")\n",
        "print(x_train_raw.head())\n",
        "print(x_train_raw.tail())\n",
        "print(\"\\nValidation:\")\n",
        "print(x_val_raw.head())\n",
        "print(x_val_raw.tail())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl5dt5eziZ0T"
      },
      "source": [
        "It is also possible to [slice](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#slicing-ranges) pandas DataFrames (and other pandas objects) same way as Python lists. You can find complete documentation of indexing and accessing elements in pandas objects [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVunhMmYix_N"
      },
      "source": [
        "print(\"Waveleghts:\")\n",
        "print(wavelengths[:8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkS5Sm0ikbm1"
      },
      "source": [
        "Class labels are stored in a [Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object (DataFrames are collections of Series). You can print it directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjqcuC9jk6sM"
      },
      "source": [
        "print(\"Train class labels:\")\n",
        "print(y_train_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2HfAHomLnk"
      },
      "source": [
        "Or get some summary using [`.value_counts()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) function. Try to change `normalize` parameter to _True_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88deYnqmmLTI"
      },
      "source": [
        "train_label_counts = y_train_raw.value_counts(normalize=False)\n",
        "val_label_counts = y_val_raw.value_counts(normalize=False)\n",
        "\n",
        "print(\"Train class labels value counts:\")\n",
        "print(train_label_counts)\n",
        "print(\"\\nValidation class labels value counts:\")\n",
        "print(val_label_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vnd5WLNrWlC"
      },
      "source": [
        "We use [plotly](https://plot.ly/python/) library for plotting visuals and [colorlover](https://github.com/plotly/colorlover) package for choosing color scales (color maps) in this tutorial. You can find various types of colormaps at [ColorBrewer](http://colorbrewer2.org)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYxtMSCHhih7"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import colorlover as cl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Ox0FWYrbig"
      },
      "source": [
        "counts = pd.DataFrame({'train': train_label_counts, 'validation': val_label_counts})\n",
        "\n",
        "fig = go.Figure()\n",
        "for i in range(counts.shape[0]):\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x = counts.columns,\n",
        "            y = counts.iloc[i],\n",
        "            name = \"Class {}\".format(i+1),\n",
        "            marker = {'color': cl.scales['12']['qual']['Paired'][i]}\n",
        "        )\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title = \"Datasets composition\",\n",
        "    barmode ='stack')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLWSuVItrHMy"
      },
      "source": [
        "For some basic statistical overview of the DataFrame you can use [`DataFrame.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) function. It gives you statistics variable-wise, but can be time consuming for larger datasets (roughly 3 mins for this validation dataset under default Google Colab runtime environment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9_tKeiBna03"
      },
      "source": [
        "print(x_val_raw.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMPw3eJCYZmC"
      },
      "source": [
        "Or you can use numpy's [statistics](https://docs.scipy.org/doc/numpy/reference/routines.statistics.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlDBrYlDa3Mi"
      },
      "source": [
        "print(\"--> min\\n{}\\n\".format(np.min(x_train_raw)))\n",
        "print(\"--> mean\\n{}\\n\".format(np.mean(x_train_raw)))\n",
        "print(\"--> max\\n{}\\n\".format(np.max(x_train_raw)))\n",
        "print(\"--> std\\n{}\\n\".format(np.std(x_train_raw)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO5uEFFbfad0"
      },
      "source": [
        "And finally let's look how our spectra look like. First we define function for plotting multiple spectra into one plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gTsmT-3Yh-C"
      },
      "source": [
        "def plot_spectra(spectra, calibration=None, title=None, labels=None,\n",
        "                 colormap=cl.scales['12']['qual']['Paired'], axes_titles=True):\n",
        "    if calibration is None:\n",
        "        calibration = np.arange(len(spectra[0]))\n",
        "    if labels is None:\n",
        "        labels = [\"class {}\".format(x+1) for x in range(len(spectra))]\n",
        "    fig = go.Figure()\n",
        "    for i in range(len(spectra)):\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x = calibration,\n",
        "                y = spectra[i],\n",
        "                name = str(labels[i]),\n",
        "                line = {'color': colormap[i % len(colormap)]},\n",
        "            )\n",
        "        )\n",
        "    fig.update_layout(\n",
        "        title = title,\n",
        "        xaxis_title = \"wavelengths [nm]\" if axes_titles else \"\",\n",
        "        yaxis_title = \"intensity [A.u.]\" if axes_titles else \"\")\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ5gri-FhUYD"
      },
      "source": [
        "# indicies of one spectrum from each class\n",
        "class_representatives = [100, 1588, 2502, 3102, 3502, 4401, 5201, 6102, 6203, 7700, 9201, 9999]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfIdEZ36IwxH"
      },
      "source": [
        "fig = plot_spectra(x_train_raw.loc[class_representatives].to_numpy(), wavelengths,\n",
        "                    \"Raw train spectra (Number of features: {})\".format(x_train_raw.shape[1]))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-rD_vPoqCVc"
      },
      "source": [
        "Explore various interactive tools that plotly offers. E.g. disable/enable specific spectrum, zoom, download plot as png etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIhOx2C_cTjn"
      },
      "source": [
        "# 3) Preprocessing\n",
        "\n",
        "It is usually not possible to use raw data directly taken from a measurement. Most of the machine learning methods need some preprocessing.\n",
        "\n",
        "The basic preprocessing routine could be composed of: data normalization, outlier detecting/filtering, data labeling (separation to classes, ...) and more.\n",
        "Here, we demonstrate simple 0-1 normalization and labeling of spectroscopic data for later classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_if9BNtcV4f"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "Generally, normalization of the data is a crucial step before you can start building (training) ML models. The central idea of the normalization is to keep variables in a selected range, to be comparable together and also between individual data samples. Unnormalized data could lead to slow (or non-) convergence of the model (e.g. vanishing/exploding gradients in ANN, ...).\n",
        "The basic advice is to keep your data with zero mean unit variance. However, this could be sometimes altered by the needs of an actual task (e.g spectroscopy: spectra are not zero centered).  \n",
        "\n",
        "In spectroscopy, there are several commonly used normalization techniques:\n",
        " - 0-1 normalization (UVN) is used for classification tasks or more general qualitative analysis, where exact line intensity is not required\n",
        "\n",
        " - normalization to total emissivity could be beneficial for simple quantitative analysis (e.g. suppressing saturation effect in a calibration curve)\n",
        "\n",
        " - normalization to internal standard = spectral line (area of the peak)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmkgiz1hp3o"
      },
      "source": [
        "from sklearn.preprocessing import normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCaaHDAn1toJ"
      },
      "source": [
        "norm = \"max\"\n",
        "x_train_norm = pd.DataFrame(normalize(x_train_raw, norm=norm), index=x_train_raw.index, columns=wavelengths)\n",
        "x_val_norm = pd.DataFrame(normalize(x_val_raw, norm=norm), index=x_val_raw.index, columns=wavelengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWsq3ADOyLOh"
      },
      "source": [
        "_Warning:_ Next cell calls `describe()` method. It can be time consuming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mUhdvoI63jZ"
      },
      "source": [
        "print(\"Raw dataset\\n{}\\n\".format(x_train_raw.head()))\n",
        "print(\"Normalized dataset\\n{}\".format(x_train_norm.head()))\n",
        "print(x_train_norm.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYdTzXQGwA7O"
      },
      "source": [
        "fig = plot_spectra(x_train_norm.loc[class_representatives].to_numpy(), wavelengths,\n",
        "                   \"Normalized train spectra\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI18v4uc85nT"
      },
      "source": [
        "## One-hot encoder\n",
        "\n",
        "In multiclass classification task, it is a good practice to convert class labels (1, 2, 3,...) into [one-hot](https://en.wikipedia.org/wiki/One-hot) labels (aka dummy). The reason is to preserve same differential between any pair of different classes. This is needed when calculating an error of a model.\n",
        "\n",
        "---\n",
        "\n",
        "_Note:_ 8 - 2 = 6 and 8 - 5 = 3 but is class 2 actually more different from class 8 than class 5? We cannot say without any additional information. With three classes encoded into one-hot encoding we obtain labels: [1, 0, 0,], [0, 1, 0] and [0, 0, 1], so their difference is always of size 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAUkiifP85LT"
      },
      "source": [
        "y_train_onehot = pd.get_dummies(y_train_raw)\n",
        "y_val_onehot = pd.get_dummies(y_val_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nbZtkKw-iW5"
      },
      "source": [
        "print(\"Numerical labels:\\n{}\\n\".format(y_train_raw))\n",
        "print(\"One-hot encoded labels:\\n{}\".format(y_train_onehot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nztwpPyAz7fl"
      },
      "source": [
        "# 4) Dimensionality reduction\n",
        "Often, in modern data science, the dimension (number of variables) of the data is very high. Combined with thousands (up to millions) of data samples it can be challenging to even process the data. Everyone can think of the increased computational cost of such data, but there are more obstacles. Our intuition and imagination are not working anymore in high-dimension and more, we cannot rely on common proved tools used in low dimension. Collectively, this is called \"curse of the dimensionality\" and a good example is not usability of euclidean distance measure in the higher dimension. \\[[wikipedia](https://en.wikipedia.org/wiki/Curse_of_dimensionality)]\n",
        "\n",
        "In modern spectroscopy, we collect large complex datasets where manual inspection (of each spectrum individually) is not possible. Thus, we need to automatically (or manually) select important spectral regions or somehow extract special features to carry useful information.\n",
        "\n",
        "Most basic (naïve) dimension reduction is cropping out unimportant spectral regions. However, this technique is often time-consuming and requires advanced spectroscopic expertise (for the decision of correct spectral regions). The goal of ML is to automatize such processes, so we demonstrate several techniques to do so. Starting from a simple peak finder algorithm to more advanced Random Forest (RF) feature selection. As an alternative, feature extraction techniques are shown. In feature extraction, contrary to the selection, we create new variables to demonstrate original ones in a more efficient way. A good example is Principal Component Analysis (PCA), where we search for new variables composed as a linear combination of original variables in a way to maximize the explained variance of the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JylC1Jd90wb-"
      },
      "source": [
        "## Cropping\n",
        "As was mentioned, cropping could be used for the selection of specific spectral regions (peaks). Here we use cropping to cut off the left and right side of spectra, where spectrometer sensitivity was low and information carried is negligible. You change the range by editing corresponding threshold variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAt2vjxENgyS"
      },
      "source": [
        "wl_idx_from = 2500\n",
        "wl_idx_to = 33500\n",
        "x_train_cropp = x_train_norm.iloc[:, wl_idx_from:wl_idx_to]\n",
        "x_val_cropp = x_val_norm.iloc[:, wl_idx_from:wl_idx_to]\n",
        "wavelengths_cropp = wavelengths[wl_idx_from:wl_idx_to]\n",
        "print(\"Train dataset (cropped):\\n{}\\n\".format(x_train_cropp))\n",
        "print(\"Cropped wavelengths:\\n{}\".format(wavelengths_cropp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z65fLoB49N1"
      },
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1w3HyE57bX"
      },
      "source": [
        "### Peak finder\n",
        "\n",
        "To implement the automatized peak finder, we have used the [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html) library. For details read the documentation. Below, we search for peaks with `prominence` threshold 0.05 in the first spectrum (`spectrum_idx = 0`) of the dataset. You can try to adjust the prominence parameter value and see the results.\n",
        "\n",
        "Later, try to explore the effect of other parameters of the  `find_peaks()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy7zRlauhuky"
      },
      "source": [
        "from scipy.signal import find_peaks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmGmr2BOZJcx"
      },
      "source": [
        "spectrum_idx = 0\n",
        "peaks, _ = find_peaks(x_train_cropp.iloc[spectrum_idx], prominence=0.05)\n",
        "\n",
        "fig = plot_spectra([x_train_cropp.iloc[spectrum_idx]], wavelengths_cropp, labels=[\"spectrum\"])\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = wavelengths_cropp[peaks],\n",
        "        y = x_train_cropp.iloc[spectrum_idx, peaks],\n",
        "        name = \"peaks\",\n",
        "        mode = \"markers\"\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title = \"Number of detected peaks in spectrum #{}: {}\".format(spectrum_idx, len(peaks)),\n",
        "    autosize = True\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Ul1w9WeVzd"
      },
      "source": [
        "Applying this algorithm to each spectrum in training data subset will allow us to obtain simple statistics of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-PvlkPQem09"
      },
      "source": [
        "peak_frequency = np.zeros(wavelengths_cropp.shape, dtype=int)\n",
        "for _, spectrum in x_train_cropp.iterrows():\n",
        "    spectrum_peaks, _ = find_peaks(spectrum, prominence=0.05)\n",
        "    for peak_location in spectrum_peaks:\n",
        "        peak_frequency[peak_location] += 1\n",
        "peak_frequency = pd.Series(peak_frequency, dtype=int)\n",
        "\n",
        "print(\"Statistics of 'votes' for peaks at particular position:\\n{}\".format(\n",
        "    peak_frequency.describe(percentiles=[.25, .30, .50, .60, .70, .75])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hwJvYGNwZ7C"
      },
      "source": [
        "Now, we may filter out \"unimportant\" peaks. The selection criterion here is the total number of votes (over all spectra in the training dataset) per selected wavelength (variable). In other words, we select only wavelengths where we detect peaks more frequently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3-B6HDhsRTM"
      },
      "source": [
        "min_votes = 45  # Based on statistics from the cell above (circa 75th percentile)\n",
        "\n",
        "peak_filter = [x >= min_votes for x in peak_frequency]\n",
        "peak_filter = pd.Series(peak_filter, dtype=bool)\n",
        "print(\"Number of wavelengths detected as peaks ('True') vs the rest ('False'):\\n{}\".format(\n",
        "    peak_filter.value_counts()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPm-M65wDpzC"
      },
      "source": [
        "We created a filter (or mask) pd.Series containing `True` at wavelength's position which should be kept and `Flase` at positions which should be filtered out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDQjc9bZhFa9"
      },
      "source": [
        "fig = plot_spectra([np.mean(x_train_cropp)], wavelengths_cropp, labels=[\"mean spectrum\"])\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = wavelengths_cropp[peak_filter.values],\n",
        "        y = np.mean(x_train_cropp)[peak_filter.values],\n",
        "        name = \"peaks\",\n",
        "        mode = \"markers\"\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title = \"Number of detected peaks (throughout whole training dataset) with mean spectrum: {}\"\n",
        "                .format(peak_filter.value_counts()[True]),\n",
        "    autosize = True\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9oF840qD420"
      },
      "source": [
        "You can see that the the number of features dropped significantly. But there still is a lot of wavelengths marked as important (peaks) that probably shouldn't be marked and rather be dropped. You can try to change parameters of the peak-finder algorithm to get better results. Don't hesitate to contact us if you find better settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR3h71ARkh3-"
      },
      "source": [
        "x_train_peaks = x_train_cropp.loc[:, peak_filter.values]\n",
        "x_val_peaks = x_val_cropp.loc[:, peak_filter.values]\n",
        "wavelengths_peaks = wavelengths_cropp[peak_filter.values]\n",
        "\n",
        "print(\"Train dataset (after peak finder): {}\".format(x_train_peaks.shape))\n",
        "print(\"Validation dataset (after peak finder): {}\".format(x_val_peaks.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c09HjPKsI957"
      },
      "source": [
        "fig = plot_spectra(x_train_peaks.loc[class_representatives].to_numpy(), wavelengths_peaks,\n",
        "            title=\"Dataset filtered by peak finder (# of features: {})\".format(x_train_peaks.shape[1]))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxZoHXOx7-lg"
      },
      "source": [
        "### Extreme Random Forest Feature Selection\n",
        "\n",
        "This method of feature selection is based on a classifier ([Extreme Random Forest / ExtraTree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html) in our case) that can natively describe importance of each feature for the classification result. We put _scikit-learn's_ [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) on top of the classifier to filter out all features that are not important during the classification and keep only those with high impact (declared by the underlaying Extreme Random Forest (ERF))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw0UxQM-ZxW8"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSdQk-BkF_xc"
      },
      "source": [
        "_Note_: Decision Trees and their derivatives such as ERF are not affected by not using one-hot encoded labels, so we can use raw labels here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0pokjU3skzc"
      },
      "source": [
        "clf = ExtraTreesClassifier(n_estimators = 200, verbose=1, n_jobs=-1)\n",
        "selector = SelectFromModel(clf, prefit=False).fit(x_train_cropp, y_train_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMLyU0ruwPVP"
      },
      "source": [
        "erf_filter = pd.Series(selector.get_support(), dtype=bool)\n",
        "print(\"Number of features (wavelengths) detected as important for classification ('True') vs the rest ('False'):\\n{}\".format(\n",
        "    erf_filter.value_counts()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ-OPBXNw8Yz"
      },
      "source": [
        "fig = plot_spectra([np.mean(x_train_cropp)], wavelengths_cropp, labels=[\"mean spectrum\"])\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = wavelengths_cropp[erf_filter.values],\n",
        "        y = np.mean(x_train_cropp)[erf_filter.values],\n",
        "        name = \"important features\",\n",
        "        mode = \"markers\"\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title = \"Number of important features: {}\".format(erf_filter.value_counts()[True]),\n",
        "    autosize = True\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrMr_diqy_Tk"
      },
      "source": [
        "x_train_erf = x_train_cropp.loc[:, erf_filter.values]\n",
        "x_val_erf = x_val_cropp.loc[:, erf_filter.values]\n",
        "wavelengths_erf = wavelengths_cropp[erf_filter.values]\n",
        "\n",
        "print(\"Train dataset (after ERF-filter): {}\".format(x_train_erf.shape))\n",
        "print(\"Validation dataset (after ERF-filter): {}\".format(x_val_erf.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lARgduNFJFFw"
      },
      "source": [
        "fig = plot_spectra(x_train_erf.loc[class_representatives].to_numpy(), wavelengths_erf,\n",
        "            title=\"Dataset filtered by ERF (# of features: {})\".format(x_train_erf.shape[1]))\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SUTbu4G4uzF"
      },
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kgFh2cVG1Hg"
      },
      "source": [
        "### Principal Component Aanalysis (PCA)\n",
        "PCA is an unsupervised technique for dimensionality reduction and visualization of the data. It is often considered as the first choice for data exploration. The basic idea behind PCA is to find new variables (principal components - PCs) as a linear combination of the original variables with a constraint to maximize the variance explained by the PC (+ orthogonality of PCs). Later, only PCs with the highest contribution to the total variance are kept while dropping the rest. This will reduce the dimension of the data and open new possibilities for visualization.\n",
        "\n",
        "PCA was proven as a very capable technique in spectroscopy (LIBS). To learn how to use PCA for data processing directly, see the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDlBdnWMi424"
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD4Ffw48EkmZ"
      },
      "source": [
        "At first we calculate the PCA (on just cropped dataset) and keep first 50 features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdX2-_uBymb0"
      },
      "source": [
        "pca = PCA(n_components=50).fit(x_train_cropp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M_z4JfwEn7Y"
      },
      "source": [
        "Then we observe a [scree plot](https://en.wikipedia.org/wiki/Scree_plot) to choose proper number of principal components to keep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mERJGWplocn-"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = [x+1 for x in range(len(pca.explained_variance_))],\n",
        "        y = pca.explained_variance_ratio_ * 100,\n",
        "        mode = \"lines+markers\",\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title = \"Explained vairance by Principal Components (scree plot)\",\n",
        "    xaxis_title = \"PC\",\n",
        "    yaxis_title = \"explained variance [%]\"\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ9MqohDxOvw"
      },
      "source": [
        "It is slightly subjective where to make a cut. You basically look for an \"elbow\". Let's use 10 PCs in our case, but 9, 11 or 12 could be used as well.\n",
        "\n",
        "---\n",
        "\n",
        "Loadings and scores are two important parts of PCA. Briefly, loadings describe how were PCs computed from original variables and scores are coordinates of samples in the new PCA-space.\n",
        "\n",
        "[Here](https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another) you can read more about loadings and their connection to eigenvectors and eigenvalues."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGZ7ApYDmU4i"
      },
      "source": [
        "pca = PCA(n_components=10)\n",
        "scores_pca_train = pca.fit_transform(x_train_cropp)\n",
        "loadings = np.array([list(np.sqrt(pca.explained_variance_[i]) * pca.components_[i]) for i in range(10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGz1Mr0DFSWE"
      },
      "source": [
        "It is possible to plot loadings same way as spectra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQu6Czaw4jth"
      },
      "source": [
        "print(\"Scores:\", scores_pca_train.shape)\n",
        "print(\"Loadings:\", loadings.shape)\n",
        "\n",
        "fig = plot_spectra(loadings, wavelengths_cropp, \"PCA loadigs\",\n",
        "            labels=[\"PC{}\".format(x+1) for x in range(loadings.shape[0])])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx0H07crE7i0"
      },
      "source": [
        "From simplified point of view we can say that spectra with high PC1 value (in `scores`) are similar to the imaginary spectrum of PC1 (in `loadings`). Negative values in loadings denotes anticorrelation (absence of peaks at particular wavelength).\n",
        "\n",
        "---\n",
        "\n",
        "With low number of features (dimensions) we can finally plot samples into a human readable way and observe some hidden patterns in the dataset. Scores is the low-dimensional representation of our dataset, so let's plot them in a plane.\n",
        "\n",
        "_Note:_ Try to change which PCs are plotted along x and y axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbK4OxJV-Vpu"
      },
      "source": [
        "x_axis = 1      # PC# along x axis\n",
        "y_axis = 2      # PC# along y axis\n",
        "\n",
        "fig = go.Figure()\n",
        "for i_class in range(1, 13):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x = scores_pca_train[y_train_raw == i_class, x_axis-1],\n",
        "            y = scores_pca_train[y_train_raw == i_class, y_axis-1],\n",
        "            mode = \"markers\",\n",
        "            name = \"Class {}\".format(i_class),\n",
        "            marker = dict(\n",
        "                size = 5,\n",
        "                color = cl.scales['12']['qual']['Paired'][i_class-1]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "fig.update_layout(\n",
        "    xaxis_title = \"PC{}\".format(x_axis),\n",
        "    yaxis_title = \"PC{}\".format(y_axis),\n",
        "    title = \"PCA scores\",\n",
        "    width = 1000,\n",
        "    height = 700,\n",
        "    legend = dict(\n",
        "        font = dict(\n",
        "            size = 14\n",
        "        ),\n",
        "        itemsizing = 'constant'\n",
        "    ),\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw99DA3XFZZA"
      },
      "source": [
        "Sometimes 2D is not enough and some pattern (e.g. two classes are [lineary separable](https://en.wikipedia.org/wiki/Linear_separability)) can be seen only by adding an extra dimension. Next scatter plot is same as the one above but in 3D. Try to rotate it, zoom or disable/enable a class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M8nN7d1y2cf"
      },
      "source": [
        "x_axis = 1      # PC# along x axis\n",
        "y_axis = 2      # PC# along y axis\n",
        "z_axis = 3      # PC# along z axis\n",
        "\n",
        "fig = go.Figure()\n",
        "for i_class in range(1, 13):\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x = scores_pca_train[y_train_raw == i_class, x_axis-1],\n",
        "            y = scores_pca_train[y_train_raw == i_class, y_axis-1],\n",
        "            z = scores_pca_train[y_train_raw == i_class, z_axis-1],\n",
        "            mode = \"markers\",\n",
        "            name = \"Class {}\".format(i_class),\n",
        "            marker = dict(\n",
        "                size = 3,\n",
        "                color = cl.scales['12']['qual']['Paired'][i_class-1]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "fig.update_layout(\n",
        "    scene = dict(\n",
        "        xaxis_title = \"PC{}\".format(x_axis),\n",
        "        yaxis_title = \"PC{}\".format(y_axis),\n",
        "        zaxis_title = \"PC{}\".format(z_axis)\n",
        "    ),\n",
        "    legend = dict(\n",
        "        font = dict(\n",
        "            size = 16\n",
        "        ),\n",
        "        itemsizing = 'constant'\n",
        "    ),\n",
        "    title = \"PCA scores\",\n",
        "    width=1000,\n",
        "    height=900\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eenQksytq0W2"
      },
      "source": [
        "pca_columns = [\"PC{}\".format(x) for x in range(1, 11)]\n",
        "x_train_pca = pd.DataFrame(scores_pca_train,\n",
        "                           index = x_train_cropp.index,\n",
        "                           columns = pca_columns)\n",
        "x_val_pca = pd.DataFrame(pca.transform(x_val_cropp),\n",
        "                         index = x_val_cropp.index,\n",
        "                         columns = pca_columns)\n",
        "\n",
        "print(\"Training dataset transformed into PCA space {}:\\n{}\\n\".format(x_train_pca.shape, x_train_pca))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN200KZJFnko"
      },
      "source": [
        "PCA also allows you to transform your scores back to the original high-dimensional space using the calculated loadings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH9Q9gg1k-XU"
      },
      "source": [
        "x_train_pca_restored = pd.DataFrame(pca.inverse_transform(x_train_pca), index=x_train_cropp.index,\n",
        "                                    columns=wavelengths_cropp)\n",
        "\n",
        "print(\"Training dataset transformed back to original space:\\n{}\".format(x_train_pca_restored))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA8sHQGAFr4j"
      },
      "source": [
        "The main point is that you are not able to obtain completely same dataset back because you have dropped some of the information and kept only 10 principal components. You can see how it affects the spectrum in the plot below.\n",
        "\n",
        "---\n",
        "\n",
        "_First 10 PCs describe only important information from the original dataset, not noise. Thus, the restored spectrum has less noise than the original one._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0OPeuH2hZQ"
      },
      "source": [
        "plot_spectra([x_train_cropp.iloc[0], x_train_pca_restored.iloc[0]], wavelengths_cropp,\n",
        "             title=\"Reconstructed spectrum form PCA (10 principal components) vs original spectrum\",\n",
        "             labels=[\"original\", \"reconstructed\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO4zEWNoG39Q"
      },
      "source": [
        "### t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "t-SNE is an unsupervised iterative method designed to find a projection into low-dimensional space (typically 2D or 3D) based on similarities and disimilarities of samples in high-dimensional space. Similar samples (close points) in high-dimensional space should be close also in low-dimensional representation. The same stands also for far points respectively.\n",
        "\n",
        "Here you can find more info about t-SNE:\n",
        "- [wikipedia](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n",
        "- [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
        "- [author's webpage](https://lvdmaaten.github.io/tsne/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjCB3NwJ9prv"
      },
      "source": [
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BE_ayQe-Ix0"
      },
      "source": [
        "Not surprisingly, t-SNE has some advantages and disadvantages compared to PCA. Disadvantages are for example:\n",
        "- Higher computation time.\n",
        "- Only one single-directed projection. It's not possible to transform back to the original space and it's not possible to use already computed mapping to project new (unseen) samples to the low-dimensional space.\n",
        "- There is significantly more parameters to optimize. t-SNE is a non-deterministic (probability based) method, so even with same parameters you can obtain different results.\n",
        "\n",
        "On the other hand, t-SNE is designed for visualization, so the result is often much \"nicer\" than using PCA. Clusters in the low-dimensional projection can be clearer and more separated from each other.\n",
        "\n",
        "_Note: Some implementations of t-SNE use PCA (with 50-100 PCs) as the first step (initialization) for speed up the computation._\n",
        "\n",
        "_Note2: We use ERF-filtered dataset here. Computing t-SNE from  higher-dimensional dataset (`_cropp` or even `_norm` would take too long)._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzkgqsFb91Bw"
      },
      "source": [
        "tsne = TSNE(n_components=2, init=\"pca\", verbose=1, n_jobs=-1)\n",
        "scores_tsne_train = tsne.fit_transform(x_train_erf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oeCIddYKGbg"
      },
      "source": [
        "print(\"t-SNE 'scores': {}\".format(scores_tsne_train.shape))\n",
        "\n",
        "fig = go.Figure()\n",
        "for i_class in range(1, 13):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x = scores_tsne_train[y_train_raw == i_class, 0],\n",
        "            y = scores_tsne_train[y_train_raw == i_class, 1],\n",
        "            mode = \"markers\",\n",
        "            name = \"Class {}\".format(i_class),\n",
        "            marker = dict(\n",
        "                size = 5,\n",
        "                color = cl.scales['12']['qual']['Paired'][i_class-1]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title = \"t-SNE space\",\n",
        "    xaxis_title = \"dimension 1\",\n",
        "    yaxis_title = \"dimension 2\",\n",
        "    width = 1000,\n",
        "    height = 800,\n",
        "    legend = dict(\n",
        "        font = dict(\n",
        "            size = 12\n",
        "        ),\n",
        "        itemsizing = 'constant'\n",
        "    ),\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADtpVHg9GHR1"
      },
      "source": [
        "It is possible to compute t-SNE for 3-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SeC0RbrKLhX"
      },
      "source": [
        "tsne_3d = TSNE(n_components=3, init=\"pca\", verbose=1, n_jobs=-1, perplexity=50)\n",
        "scores_tsne_train_3d = tsne_3d.fit_transform(x_train_erf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9BNfUzsH2mM"
      },
      "source": [
        "fig = go.Figure()\n",
        "for i_class in range(1, 13):\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x = scores_tsne_train_3d[y_train_raw == i_class, 0],\n",
        "            y = scores_tsne_train_3d[y_train_raw == i_class, 1],\n",
        "            z = scores_tsne_train_3d[y_train_raw == i_class, 2],\n",
        "            mode = \"markers\",\n",
        "            name = \"Class {}\".format(i_class),\n",
        "            marker = dict(\n",
        "                size = 3,\n",
        "                color = cl.scales['12']['qual']['Paired'][i_class-1]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title = \"t-SNE (train dataset) space\",\n",
        "    width = 1000,\n",
        "    height = 800,\n",
        "    legend = dict(\n",
        "        font = dict(\n",
        "            size = 12\n",
        "        ),\n",
        "        itemsizing = 'constant'\n",
        "    ),\n",
        "\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o3H8cvHFZJI"
      },
      "source": [
        "As mentioned above, it's not possible to simply use the same already computed mapping and add new points (e.g. validation set) to the plot. You have to compute t-SNE separately for validation set. Due to nondeterminism of the method it's possible that the result will be different than the one computed from training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbnMnKSODAv"
      },
      "source": [
        "val_tsne = TSNE(n_components=2, init=\"pca\", verbose=1, n_jobs=-1)\n",
        "scores_tsne_val = tsne.fit_transform(x_val_erf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUGmi46GT_l"
      },
      "source": [
        "You can compare the plot below with the first one (training dataset - 2D). They look similar in general, but you can't combine them into one using same coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KX7ohtFOUXJ"
      },
      "source": [
        "print(\"t-SNE validation 'scores': {}\".format(scores_tsne_val.shape))\n",
        "\n",
        "fig = go.Figure()\n",
        "for i_class in range(NUM_CLASSES):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x = scores_tsne_val[y_val_raw == i_class, 0],\n",
        "            y = scores_tsne_val[y_val_raw == i_class, 1],\n",
        "            mode = \"markers\",\n",
        "            name = \"Class {}\".format(i_class),\n",
        "            marker = dict(\n",
        "                size = 5,\n",
        "                color = colormap[i_class]\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "fig.update_layout(\n",
        "    title = \"t-SNE (validation dataset) space\",\n",
        "    xaxis_title = \"dimension 1\",\n",
        "    yaxis_title = \"dimension 2\",\n",
        "    width = 1000,\n",
        "    height = 800,\n",
        "    legend = dict(\n",
        "        font = dict(\n",
        "            size = 12\n",
        "        ),\n",
        "        itemsizing = 'constant'\n",
        "    ),\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1cXDWtBHK7n"
      },
      "source": [
        "# 5) Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRsdtgcCbEeR"
      },
      "source": [
        "## Support Vector Machines (SVM)\n",
        "\n",
        "First classifier we use in this notebook is SVM. This method tries to find a hyperplane (a line in 2D space) that separates two classes. In advance it's a hyperplane such that margins between it and the closest data points are maximal [[wiki](https://en.wikipedia.org/wiki/Support-vector_machine)][[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vol8fpFwrS_"
      },
      "source": [
        "### Hyper-parameter grid search\n",
        "\n",
        "SVM has several parameters that has to be optimized for every task (for details see documentation). A naive and computationally demanding (but safe) optimization method is a grid search combined with a cross validation.\n",
        "\n",
        "[Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html) (CV) is a very usefull method and definitely a good practice when you evaluate your model. Briefly, you split your training dataset into $k$ disjoint subsets and train $k$ models separately. In every so-called _fold_ you use one of the subsets for validation (different one in every fold) and $k$-1 subsets for training. The result is then obtained from set of results (one for every fold) as mean of them. It's less affected by ranndomness during dataset selection.\n",
        "\n",
        "The combination of CV with grid search is straightforward. At first you define the grid of parameters that should be used. Then you compute mean accuracy of every possible combination of parameters using CV. In the end you choose the best combination of parameters and train the model at the whole dataset.\n",
        "\n",
        "Grid Search CV [tutorial](https://scikit-learn.org/stable/modules/grid_search.html) and [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsKxtp69C9V6"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg5Erg8MGxBU"
      },
      "source": [
        "It's time consuming to go through the whole grid, so we already pruned it. You can try different parts of the grid or even whole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jit2RGHgD_rV"
      },
      "source": [
        "parameters = [\n",
        "    # {'kernel': ['linear'], 'C': [1, 10, 1000, 10000]},\n",
        "    {'kernel': ['rbf'], 'C': [10e5, 10e6, 10e10],\n",
        "     'gamma': ['scale', 'auto', 0.01, 0.001]},\n",
        "    #{'kernel': ['poly'], 'C': [1, 10, 1000, 10000],\n",
        "    # 'gamma': ['scale', 'auto', 0.01, 0.001], 'degree': [3, 4, 5]}\n",
        "]\n",
        "svm = SVC()\n",
        "gscv = GridSearchCV(svm, parameters, n_jobs=-1, verbose=1).fit(x_train_pca, y_train_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqcw7cCUnWhI"
      },
      "source": [
        "print(\"Best params: {}\".format(gscv.best_params_))\n",
        "print(\"Best score: {:.2f} %\".format(gscv.best_score_ * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvtHJp-4wgZf"
      },
      "source": [
        "You can save the grid search results for better analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2qR5BzmwWGJ"
      },
      "source": [
        "cv_results = pd.DataFrame(gscv.cv_results_)\n",
        "cv_results.to_csv(\"svm_gridsearch.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8c4LGySwxYT"
      },
      "source": [
        "### Final training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASPZEv0twptD"
      },
      "source": [
        "svm = SVC(kernel='rbf', C=1e7, gamma=0.001).fit(x_train_pca, y_train_raw)\n",
        "y_train_pred = svm.predict(x_train_pca)\n",
        "y_val_pred = svm.predict(x_val_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR9g5DwAG01B"
      },
      "source": [
        "There are basically two ways how to evaluate a model - [cross validation](https://machinelearningmastery.com/k-fold-cross-validation/) or _train-val-test_ split. Cross validation is especially handy when your dataset is not large enough and hard splitting would make one of the datasets (train, validation or test) too small. We use _train-val_ split in this tutorial. Test dataset would be later used for final evaluation and comparison with other contestants in the EMSLIBS contest, our validation dataset is used for evaluation of different models (types, architectures, preprocessing used, etc.).\n",
        "\n",
        "In this case it is crucial not to \"contaminate\" the validation (or test) set by using it during any preprocessing or training step. You should put it aside as soon as you create it, use it only for evaluation of your model and you must not declare your model's performance on any dataset you used during training if you do not use CV.\n",
        "\n",
        "_Note: Even selecting optimal number of epochs can be considered as contamination. From that purist point of view, what is done in chapter **Multilayer perceptron (MLP)** is a bad practice. We should use another 'test' dataset for declaring the performance, if we find optimal number of epochs using validation dataset. Typical datasets when you don't use CV are: 'training' - used for exploration and training; 'validation' - used for optimizing parameters of models (number of epochs, number of layers and neurons, kernels and their parameters, etc.) and their comparison (SVM vs MLP, different MLP topologies, etc.); 'test' - for evaluation of your final and optimized model (never seen by the model before), this should be your declared performance._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRVKNBDZCYme"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import plotly.figure_factory as ff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exCqtMmWHhIs"
      },
      "source": [
        "This method plots a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). For more info see [this](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/) blog or [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eK-sgZiXCC-"
      },
      "source": [
        "def plot_conf_matrix(y_true, y_pred, normalize, dataset_name):\n",
        "    z = confusion_matrix(y_true, y_pred, normalize=normalize)\n",
        "    fig = ff.create_annotated_heatmap (\n",
        "        z,\n",
        "        colorscale='Blues',\n",
        "        x = [str(i) for i in range(1, 13)],\n",
        "        y = [str(i) for i in range(1, 13)],\n",
        "        showscale=True,\n",
        "        annotation_text = np.around(z, decimals=2),\n",
        "        hoverinfo='z')\n",
        "    title = \"Confusion matrix - '\" + dataset_name + \"'\"\n",
        "    if normalize == \"true\":\n",
        "        title += \" (normalized)\"\n",
        "    fig.update_layout(\n",
        "        title = title,\n",
        "        xaxis_title = {\n",
        "            'text': \"predicted class\",\n",
        "            'font': {'size': 10}\n",
        "        },\n",
        "        yaxis_title = {\n",
        "            'text': \"true class\",\n",
        "            'font': {'size': 10}\n",
        "        },\n",
        "        xaxis = {\n",
        "            'linecolor': 'black',\n",
        "            'mirror': True\n",
        "        },\n",
        "        yaxis = {\n",
        "            'linecolor': 'black',\n",
        "            'mirror': True\n",
        "        },\n",
        "        width = 500,\n",
        "        height = 500\n",
        "    )\n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRpgzDcjICli"
      },
      "source": [
        "plot_conf_matrix(y_train_raw, y_train_pred, normalize=\"true\", dataset_name=\"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMlrkV0HsBT_"
      },
      "source": [
        "plot_conf_matrix(y_train_raw, y_train_pred, normalize=None, dataset_name='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n57KnwdICmR"
      },
      "source": [
        "print(\"Training dataset classification report:\\n{}\".format(classification_report(y_train_raw, y_train_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-sd55rmsd5q"
      },
      "source": [
        "plot_conf_matrix(y_val_raw, y_val_pred, normalize=\"true\", dataset_name=\"val\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGkZgxgjsvE-"
      },
      "source": [
        "plot_conf_matrix(y_val_raw, y_val_pred, normalize=None, dataset_name=\"val\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhZ95n1ctAEn"
      },
      "source": [
        "print(\"Validation dataset classification report:\\n{}\".format(classification_report(y_val_raw, y_val_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYnuLQzps2-Y"
      },
      "source": [
        "Due to basic random _train-val_ split the results are pretty good. If you create a validation dataset more similar to the original test set (see the mentioned [paper](https://www.nature.com/articles/s41597-020-0396-8)), the model would probably tend to overfit.\n",
        "\n",
        "---\n",
        "\n",
        "Overfitted model has perfect accuracy on training dataset, but poor on validation dataset. There are many reasons why your model can overfit. One of them is that a model has problem with all classes that have low frequency in the training dataset. This is not surprising and it can be solved by [oversampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEPR6EGKHRX3"
      },
      "source": [
        "## Multilayer perceptron (MLP)\n",
        "\n",
        "Feedforward neural network or as called Multilayer perceptron ([MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron)) is a type of Neural network used mainly for supervised tasks (classification or regression).\n",
        "\n",
        "It is built from layers of neurons (simple computation unit) stacked one on each other. Every layer is fully connected with its neighbors (one lower and one upper layer). These connections are called weights and they are the parameters that are \"learned\" during training mode. For more info about neural networks see [Deep Learning Book](https://www.deeplearningbook.org/).\n",
        "\n",
        "There are several ways how to easily create a neural network in Python. You can try [scikit-learn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network) used above or one of Python frameworks specialized on neural networks such as ([Theano](http://deeplearning.net/software/theano/index.html), [PyTorch](https://pytorch.org/) or [Keras](https://keras.io/)). We use Keras combined with [TensorFlow](https://www.tensorflow.org/) as a backend in this tutorial (can be combined also with Theano)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDR-9tl2Tk_F"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, InputLayer\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpKFWTygwN-f"
      },
      "source": [
        "Here we create a simple MLP with 256-64-32-12 topology and train it on ERF dataset (filtered using Extreme Random Forest). Let's use a bigger number of epochs (epoch = round; all training samples are seen by the net once), e.g. 30 and see how accuracy evolved during the time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDh5xkvoezjJ"
      },
      "source": [
        "model = Sequential(name=\"MLP\")\n",
        "model.add(Dense(256, activation='relu', input_shape=(x_train_erf.shape[1],), name=\"hidden_1\"))\n",
        "model.add(Dropout(rate=0.2, name=\"dropout_1\"))\n",
        "model.add(Dense(64, activation='relu', name=\"hidden_2\"))\n",
        "model.add(Dropout(rate=0.2, name=\"dropout_2\"))\n",
        "model.add(Dense(32, activation='relu', name='hidden_3'))\n",
        "model.add(Dropout(rate=0.2, name='dropout_3'))\n",
        "model.add(Dense(12, activation='softmax', name=\"output\"))\n",
        "model.build()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=categorical_crossentropy,\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "n_epochs = 50\n",
        "history_obj = model.fit(x_train_erf, y_train_onehot,\n",
        "                        batch_size=16,\n",
        "                        epochs=n_epochs,\n",
        "                        verbose=1,\n",
        "                        validation_data=(x_val_erf, y_val_onehot))\n",
        "history = pd.DataFrame(history_obj.history, index=[i+1 for i in range(n_epochs)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9jGAsVFpi6j"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = history.index,\n",
        "        y = history[\"accuracy\"],\n",
        "        mode = \"lines+markers\",\n",
        "        name = \"Train\"\n",
        "    )\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x = history.index,\n",
        "        y = history[\"val_accuracy\"],\n",
        "        mode = \"lines+markers\",\n",
        "        name = \"Validation\"\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title = \"Model Accuracy\",\n",
        "    xaxis_title = \"epochs\",\n",
        "    yaxis_title = \"accuracy\"\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRMau3s-3eX3"
      },
      "source": [
        "If you optimize your model too much to the training dataset, it is possible you fit it too much to it. It describes noise and some dataset-specific features instead of general patterns. This is called overfitting and you should always try to avoid this. In the image above, find the point where validation accuracy is still increasing and set the number of epochs to this number. Keep in mind that learning process is nondeterministic and several runs with totally same parameters can give different results.\n",
        "\n",
        "_Note: Neural networks are very overfitting-prone_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9HJiZckfF4B"
      },
      "source": [
        "y_train_pred = np.argmax(model.predict(x_train_erf), axis=-1) + 1\n",
        "y_val_pred = np.argmax(model.predict(x_val_erf), axis=-1) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSpB5kVs6mA2"
      },
      "source": [
        "plot_conf_matrix(y_train_raw, y_train_pred, normalize=\"true\", dataset_name=\"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK7iFDKs6rpN"
      },
      "source": [
        "plot_conf_matrix(y_train_raw, y_train_pred, normalize=None, dataset_name='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFaHE197IfYr"
      },
      "source": [
        "print(\"Training dataset classification report:\\n{}\".format(classification_report(y_train_raw, y_train_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTDEex74IfY2"
      },
      "source": [
        "plot_conf_matrix(y_val_raw, y_val_pred, normalize='true', dataset_name='val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXyDCDwzIfZA"
      },
      "source": [
        "plot_conf_matrix(y_val_raw, y_val_pred, normalize=None, dataset_name='val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTV-T29wIfZK"
      },
      "source": [
        "print(\"Validation dataset classification report:\\n{}\".format(classification_report(y_val_raw, y_val_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}